{
  "timestamp": "2024-01-19 18:50:42.668146",
  "type": "python",
  "version": "1.0",
  "name": "Python Test Suite",
  "description": "Checks subtests and graphics",
  "status": "COMPLETED",
  "result": "FAILED",
  "resultMessage": "Some or all Tests failed",
  "details": null,
  "duration": 3.071523427963257,
  "executionDurationReference": 0.007042646408081055,
  "executionDurationStudent": 0.015700101852416992,
  "environment": {
    "Python": "3.10.11",
    "Platform": "Windows-10-10.0.19045-SP0",
    "Packages": {
      "pytest": "7.4.3",
      "pluggy": "1.3.0"
    },
    "Plugins": {
      "json-report": "1.5.0",
      "metadata": "3.0.0"
    }
  },
  "properties": null,
  "debug": null,
  "exitcode": "ExitCode.TESTS_FAILED",
  "summary": {
    "total": 3,
    "success": 0,
    "failed": 1,
    "skipped": 1,
    "timedout": 1
  },
  "tests": [
    {
      "type": "variable",
      "name": "Test timeout",
      "description": null,
      "setup": null,
      "teardown": null,
      "status": "COMPLETED",
      "result": "TIMEDOUT",
      "resultMessage": "Tests timedout",
      "details": null,
      "executionDurationReference": 0.0,
      "executionDurationStudent": 0.0,
      "summary": {
        "total": 1,
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "timedout": 1
      },
      "tests": [
        {
          "name": "var1",
          "status": "COMPLETED",
          "result": "TIMEDOUT",
          "resultMessage": "Test timedout",
          "details": null,
          "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x0000022DC2357130>, request = <FixtureRequest for <Function test_entrypoint[testcases0]>>\nrecord_property = <function record_property.<locals>.append_property at 0x0000022DC235A290>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000022DC2354A90>\ntestcases = (0, 0)\n\n    def test_entrypoint(self, request, record_property, monkeymodule, testcases):\n        self.x = self.x + 2\n        idx_main, idx_sub = testcases\n    \n        report: any = request.config.stash[report_key][\"report\"]\n        testsuite: CodeAbilityTestSuite = request.config.stash[report_key][\"testsuite\"]\n        specification: CodeAbilitySpecification = request.config.stash[report_key][\"specification\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        if not check_success_dependency(report, main.successDependency):\n            pytest.skip(f\"Dependency {main.successDependency} not satisfied\")\n    \n        dir_reference = specification.testInfo.referenceDirectory\n        dir_student = specification.testInfo.studentDirectory\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n    \n        #not needed here:\n        #verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n        #competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        \"\"\" Get solutions, measure execution time \"\"\"\n        try:\n>           solution_student, exec_time_student = get_solution(monkeymodule, specification, id, main, Solution.student, store_graphics_artefacts)\n\nI:\\PYTHON\\catester\\catester\\tests\\test_class.py:256: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmm = <_pytest.monkeypatch.MonkeyPatch object at 0x0000022DC2354A90>\nspecification = CodeAbilitySpecification(testInfo=CodeAbilityTestInfo(studentDirectory='i:\\\\PYTHON\\\\catester\\\\examples\\\\ex2\\\\student',...\\\\output', artefactDirectory='i:\\\\PYTHON\\\\catester\\\\examples\\\\ex2\\\\artefacts', studentTestCounter=2, testVersion='v1'))\nid = '1'\nmain = CodeAbilityTestCollection(qualification=None, relativeTolerance=None, absoluteTolerance=None, allowedOccuranceRange=No...one, successMessage=None, verbosity=None, name='var1', value=1, evalString=None, pattern=None, countRequirement=None)])\nwhere = <Solution.student: 'student'>, store_graphics = False\n\n    def get_solution(mm, specification: CodeAbilitySpecification, id, main: CodeAbilityTestCollection, where: Solution, store_graphics):\n        \"\"\"Calculate solution if not yet exists\"\"\"\n        exec_time = 0\n        if not \"solutions\" in globals():\n            globals()[\"solutions\"] = {}\n        if not id in globals()[\"solutions\"]:\n            globals()[\"solutions\"][id] = {}\n        if not where in globals()[\"solutions\"][id]:\n            test_info = specification.testInfo\n            test_directory = test_info.testDirectory\n            artefact_directory = test_info.artefactDirectory\n            _dir = test_info.studentDirectory if where == Solution.student else test_info.referenceDirectory\n    \n            type = main.type\n            entry_point = main.entryPoint\n            setup_code = get_property_as_list(main.setUpCode)\n            teardown_code = get_property_as_list(main.tearDownCode)\n            setup_code_dependency = main.setUpCodeDependency\n    \n            \"\"\" remember old working directory \"\"\"\n            dir_old = os.getcwd()\n    \n            \"\"\" add test-directory to paths \"\"\"\n            sys.path.append(test_directory)\n    \n            \"\"\" change into solution-directory student | reference \"\"\"\n            os.chdir(_dir)\n    \n            \"\"\" close all open figures \"\"\"\n            plt.close(\"all\")\n    \n            \"\"\" seed the random generator \"\"\"\n            random.seed(1)\n    \n            \"\"\" Override/Disable certain methods \"\"\"\n            #mm.setattr(random, \"seed\", lambda *x: None)\n            #mm.setattr(os, \"getcwd\", lambda: \"xxx\")\n            #mm.setattr(time, \"sleep\", lambda x: None)\n            #mm.setattr(time, \"time\", lambda: 999)\n            mm.setattr(plt, \"show\", lambda *x: None)\n    \n            \"\"\" start solution with empty namespace \"\"\"\n            namespace = {}\n    \n            if setup_code_dependency is not None:\n                \"\"\" start solution with prior solution \"\"\"\n                try:\n                    namespace = globals()[\"solutions\"][setup_code_dependency][where]\n                except Exception as e:\n                    print(f\"Exception: setUpCodeDependency {setup_code_dependency} not found\")\n                    print(e)\n                    raise\n    \n            if entry_point is not None:\n                \"\"\" try execute the solution \"\"\"\n                file = os.path.join(_dir, entry_point)\n                if not os.path.exists(file):\n                    if where == Solution.student:\n                        \"\"\" only raise if student entry point is not found \"\"\"\n                        raise FileNotFoundError(f\"entryPoint {entry_point} not found\")\n                else:\n                    \"\"\" measure execution time \"\"\"\n                    start_time = time.time()\n                    try:\n                        result = execute_file(file, namespace, timeout=1)\n                        if result == None:\n                            print(f\"TimeoutError: execute_file {file} failed\")\n>                           raise TimeoutError()\nE                           TimeoutError\n\nI:\\PYTHON\\catester\\catester\\tests\\test_class.py:137: TimeoutError"
        }
      ]
    },
    {
      "type": "variable",
      "name": "Test variables",
      "description": null,
      "setup": null,
      "teardown": null,
      "status": "COMPLETED",
      "result": "FAILED",
      "resultMessage": "Test Basic 1 Tests failed",
      "details": null,
      "executionDurationReference": 0.007042646408081055,
      "executionDurationStudent": 0.015700101852416992,
      "summary": {
        "total": 3,
        "success": 2,
        "failed": 1,
        "skipped": 0,
        "timedout": 0
      },
      "tests": [
        {
          "name": "var1",
          "status": "COMPLETED",
          "result": "FAILED",
          "resultMessage": "var1 Test failed",
          "details": null,
          "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x0000022DC23571F0>, request = <FixtureRequest for <Function test_entrypoint[testcases1]>>\nrecord_property = <function record_property.<locals>.append_property at 0x0000022DC235A320>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000022DC22E3760>\ntestcases = (1, 0)\n\n    def test_entrypoint(self, request, record_property, monkeymodule, testcases):\n        self.x = self.x + 2\n        idx_main, idx_sub = testcases\n    \n        report: any = request.config.stash[report_key][\"report\"]\n        testsuite: CodeAbilityTestSuite = request.config.stash[report_key][\"testsuite\"]\n        specification: CodeAbilitySpecification = request.config.stash[report_key][\"specification\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        if not check_success_dependency(report, main.successDependency):\n            pytest.skip(f\"Dependency {main.successDependency} not satisfied\")\n    \n        dir_reference = specification.testInfo.referenceDirectory\n        dir_student = specification.testInfo.studentDirectory\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n    \n        #not needed here:\n        #verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n        #competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        \"\"\" Get solutions, measure execution time \"\"\"\n        try:\n            solution_student, exec_time_student = get_solution(monkeymodule, specification, id, main, Solution.student, store_graphics_artefacts)\n            record_property(\"exec_time_student\", exec_time_student)\n            solution_reference, exec_time_reference = get_solution(monkeymodule, specification, id, main, Solution.reference, store_graphics_artefacts)\n            record_property(\"exec_time_reference\", exec_time_reference)\n        except TimeoutError as e:\n            record_property(\"timeout\", True)\n            raise\n    \n        \"\"\" if test is graphics => get saved graphics object as solution \"\"\"\n        if testtype == TypeEnum.graphics:\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\n            TypeEnum.variable,\n            TypeEnum.graphics,\n            TypeEnum.error,\n            TypeEnum.warning,\n            TypeEnum.help,\n        ]:\n            \"\"\" get the student value \"\"\"\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                \"\"\" value not found, try eval \"\"\"\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == QualificationEnum.verifyEqual:\n                \"\"\" get the reference value \"\"\"\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                \"\"\" assert variable-type \"\"\"\n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                \"\"\" assert variable-value \"\"\"\n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n>                   assert val_student == val_reference, failure_msg\nE                   AssertionError: Variable var1 has incorrect value\nE                   assert '123' == '1'\nE                     - 1\nE                     + 123\n\nI:\\PYTHON\\catester\\catester\\tests\\test_class.py:312: AssertionError"
        },
        {
          "name": "var2",
          "status": "COMPLETED",
          "result": "PASSED",
          "resultMessage": "var2 Test passed",
          "details": null,
          "longrepr": null
        },
        {
          "name": "var3",
          "status": "COMPLETED",
          "result": "PASSED",
          "resultMessage": "var3 Test passed",
          "details": null,
          "longrepr": null
        }
      ]
    },
    {
      "type": "variable",
      "name": "Test crash",
      "description": null,
      "setup": null,
      "teardown": null,
      "status": "COMPLETED",
      "result": "SKIPPED",
      "resultMessage": "Tests skipped",
      "details": null,
      "executionDurationReference": 0.0,
      "executionDurationStudent": 0.0,
      "summary": {
        "total": 1,
        "success": 0,
        "failed": 0,
        "skipped": 1,
        "timedout": 0
      },
      "tests": [
        {
          "name": "var1",
          "status": "COMPLETED",
          "result": "SKIPPED",
          "resultMessage": "Test skipped",
          "details": null,
          "longrepr": [
            "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
            224,
            "Skipped: Dependency ['0', '1'] not satisfied"
          ]
        }
      ]
    }
  ],
  "_json_report": {
    "created": 1705686645.73967,
    "duration": 3.071523427963257,
    "exitcode": 1,
    "root": "i:\\PYTHON\\catester\\catester",
    "environment": {},
    "summary": {
      "failed": 2,
      "passed": 2,
      "skipped": 1,
      "total": 5,
      "collected": 5
    },
    "collectors": [
      {
        "nodeid": "",
        "outcome": "passed",
        "result": [
          {
            "nodeid": "model/__init__.py",
            "type": "Package"
          },
          {
            "nodeid": "tests/__init__.py",
            "type": "Package"
          }
        ]
      },
      {
        "nodeid": "model/__init__.py",
        "outcome": "passed",
        "result": []
      },
      {
        "nodeid": "tests/test_class.py::CodeabilityPythonTest",
        "outcome": "passed",
        "result": [
          {
            "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases0]",
            "type": "Function",
            "lineno": 211
          },
          {
            "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases1]",
            "type": "Function",
            "lineno": 211
          },
          {
            "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases2]",
            "type": "Function",
            "lineno": 211
          },
          {
            "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases3]",
            "type": "Function",
            "lineno": 211
          },
          {
            "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases4]",
            "type": "Function",
            "lineno": 211
          }
        ]
      },
      {
        "nodeid": "tests/test_class.py",
        "outcome": "passed",
        "result": [
          {
            "nodeid": "tests/test_class.py::CodeabilityPythonTest",
            "type": "Class"
          }
        ]
      },
      {
        "nodeid": "tests/__init__.py",
        "outcome": "passed",
        "result": [
          {
            "nodeid": "tests/test_class.py",
            "type": "Module"
          }
        ]
      }
    ],
    "tests": [
      {
        "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases0]",
        "lineno": 211,
        "outcome": "failed",
        "keywords": [
          "test_entrypoint[testcases0]",
          "testcases0",
          "CodeabilityPythonTest",
          "test_class.py",
          "tests/__init__.py",
          "catester"
        ],
        "setup": {
          "duration": 0.0010359000880271196,
          "outcome": "passed"
        },
        "call": {
          "duration": 2.0126289000036195,
          "outcome": "failed",
          "crash": {
            "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
            "lineno": 137,
            "message": "TimeoutError"
          },
          "traceback": [
            {
              "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
              "lineno": 256,
              "message": ""
            },
            {
              "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
              "lineno": 137,
              "message": "TimeoutError"
            }
          ],
          "stdout": "TimeoutError: execute_file i:\\PYTHON\\catester\\examples\\ex2\\student\\timeout.py failed\nException: execute_file i:\\PYTHON\\catester\\examples\\ex2\\student\\timeout.py failed\n",
          "log": [
            {
              "name": "stopit",
              "msg": "Code block execution exceeded 1 seconds timeout",
              "args": null,
              "levelname": "WARNING",
              "levelno": 30,
              "pathname": "I:\\PYTHON\\catester\\.venv\\lib\\site-packages\\stopit\\utils.py",
              "filename": "utils.py",
              "module": "utils",
              "exc_info": null,
              "exc_text": "Traceback (most recent call last):\n  File \"I:\\PYTHON\\catester\\.venv\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n    result = func(*args, **kwargs)\n  File \"I:\\PYTHON\\catester\\catester\\tests\\test_class.py\", line 40, in execute_file\n    execute_code(file.read(), filename, namespace)\n  File \"I:\\PYTHON\\catester\\catester\\tests\\test_class.py\", line 31, in execute_code\n    exec(compile(code, filename, \"exec\"), namespace)\n  File \"i:\\PYTHON\\catester\\examples\\ex2\\student\\timeout.py\", line 3, in <module>\n    time.sleep(2)\nstopit.utils.TimeoutException",
              "stack_info": null,
              "lineno": 81,
              "funcName": "__exit__",
              "created": 1705686645.4891577,
              "msecs": 489.0,
              "relativeCreated": 3057.09171295166,
              "thread": 16480,
              "threadName": "MainThread",
              "processName": "MainProcess",
              "process": 15104
            }
          ],
          "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x0000022DC2357130>, request = <FixtureRequest for <Function test_entrypoint[testcases0]>>\nrecord_property = <function record_property.<locals>.append_property at 0x0000022DC235A290>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000022DC2354A90>\ntestcases = (0, 0)\n\n    def test_entrypoint(self, request, record_property, monkeymodule, testcases):\n        self.x = self.x + 2\n        idx_main, idx_sub = testcases\n    \n        report: any = request.config.stash[report_key][\"report\"]\n        testsuite: CodeAbilityTestSuite = request.config.stash[report_key][\"testsuite\"]\n        specification: CodeAbilitySpecification = request.config.stash[report_key][\"specification\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        if not check_success_dependency(report, main.successDependency):\n            pytest.skip(f\"Dependency {main.successDependency} not satisfied\")\n    \n        dir_reference = specification.testInfo.referenceDirectory\n        dir_student = specification.testInfo.studentDirectory\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n    \n        #not needed here:\n        #verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n        #competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        \"\"\" Get solutions, measure execution time \"\"\"\n        try:\n>           solution_student, exec_time_student = get_solution(monkeymodule, specification, id, main, Solution.student, store_graphics_artefacts)\n\nI:\\PYTHON\\catester\\catester\\tests\\test_class.py:256: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmm = <_pytest.monkeypatch.MonkeyPatch object at 0x0000022DC2354A90>\nspecification = CodeAbilitySpecification(testInfo=CodeAbilityTestInfo(studentDirectory='i:\\\\PYTHON\\\\catester\\\\examples\\\\ex2\\\\student',...\\\\output', artefactDirectory='i:\\\\PYTHON\\\\catester\\\\examples\\\\ex2\\\\artefacts', studentTestCounter=2, testVersion='v1'))\nid = '1'\nmain = CodeAbilityTestCollection(qualification=None, relativeTolerance=None, absoluteTolerance=None, allowedOccuranceRange=No...one, successMessage=None, verbosity=None, name='var1', value=1, evalString=None, pattern=None, countRequirement=None)])\nwhere = <Solution.student: 'student'>, store_graphics = False\n\n    def get_solution(mm, specification: CodeAbilitySpecification, id, main: CodeAbilityTestCollection, where: Solution, store_graphics):\n        \"\"\"Calculate solution if not yet exists\"\"\"\n        exec_time = 0\n        if not \"solutions\" in globals():\n            globals()[\"solutions\"] = {}\n        if not id in globals()[\"solutions\"]:\n            globals()[\"solutions\"][id] = {}\n        if not where in globals()[\"solutions\"][id]:\n            test_info = specification.testInfo\n            test_directory = test_info.testDirectory\n            artefact_directory = test_info.artefactDirectory\n            _dir = test_info.studentDirectory if where == Solution.student else test_info.referenceDirectory\n    \n            type = main.type\n            entry_point = main.entryPoint\n            setup_code = get_property_as_list(main.setUpCode)\n            teardown_code = get_property_as_list(main.tearDownCode)\n            setup_code_dependency = main.setUpCodeDependency\n    \n            \"\"\" remember old working directory \"\"\"\n            dir_old = os.getcwd()\n    \n            \"\"\" add test-directory to paths \"\"\"\n            sys.path.append(test_directory)\n    \n            \"\"\" change into solution-directory student | reference \"\"\"\n            os.chdir(_dir)\n    \n            \"\"\" close all open figures \"\"\"\n            plt.close(\"all\")\n    \n            \"\"\" seed the random generator \"\"\"\n            random.seed(1)\n    \n            \"\"\" Override/Disable certain methods \"\"\"\n            #mm.setattr(random, \"seed\", lambda *x: None)\n            #mm.setattr(os, \"getcwd\", lambda: \"xxx\")\n            #mm.setattr(time, \"sleep\", lambda x: None)\n            #mm.setattr(time, \"time\", lambda: 999)\n            mm.setattr(plt, \"show\", lambda *x: None)\n    \n            \"\"\" start solution with empty namespace \"\"\"\n            namespace = {}\n    \n            if setup_code_dependency is not None:\n                \"\"\" start solution with prior solution \"\"\"\n                try:\n                    namespace = globals()[\"solutions\"][setup_code_dependency][where]\n                except Exception as e:\n                    print(f\"Exception: setUpCodeDependency {setup_code_dependency} not found\")\n                    print(e)\n                    raise\n    \n            if entry_point is not None:\n                \"\"\" try execute the solution \"\"\"\n                file = os.path.join(_dir, entry_point)\n                if not os.path.exists(file):\n                    if where == Solution.student:\n                        \"\"\" only raise if student entry point is not found \"\"\"\n                        raise FileNotFoundError(f\"entryPoint {entry_point} not found\")\n                else:\n                    \"\"\" measure execution time \"\"\"\n                    start_time = time.time()\n                    try:\n                        result = execute_file(file, namespace, timeout=1)\n                        if result == None:\n                            print(f\"TimeoutError: execute_file {file} failed\")\n>                           raise TimeoutError()\nE                           TimeoutError\n\nI:\\PYTHON\\catester\\catester\\tests\\test_class.py:137: TimeoutError"
        },
        "user_properties": [
          {
            "timeout": true
          }
        ],
        "teardown": {
          "duration": 0.0003923999611288309,
          "outcome": "passed"
        }
      },
      {
        "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases1]",
        "lineno": 211,
        "outcome": "failed",
        "keywords": [
          "test_entrypoint[testcases1]",
          "testcases1",
          "CodeabilityPythonTest",
          "test_class.py",
          "tests/__init__.py",
          "catester"
        ],
        "setup": {
          "duration": 0.0004902000073343515,
          "outcome": "passed"
        },
        "call": {
          "duration": 0.012418399914167821,
          "outcome": "failed",
          "crash": {
            "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
            "lineno": 312,
            "message": "AssertionError: Variable var1 has incorrect value\nassert '123' == '1'\n  - 1\n  + 123"
          },
          "traceback": [
            {
              "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
              "lineno": 312,
              "message": "AssertionError"
            }
          ],
          "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x0000022DC23571F0>, request = <FixtureRequest for <Function test_entrypoint[testcases1]>>\nrecord_property = <function record_property.<locals>.append_property at 0x0000022DC235A320>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000022DC22E3760>\ntestcases = (1, 0)\n\n    def test_entrypoint(self, request, record_property, monkeymodule, testcases):\n        self.x = self.x + 2\n        idx_main, idx_sub = testcases\n    \n        report: any = request.config.stash[report_key][\"report\"]\n        testsuite: CodeAbilityTestSuite = request.config.stash[report_key][\"testsuite\"]\n        specification: CodeAbilitySpecification = request.config.stash[report_key][\"specification\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        if not check_success_dependency(report, main.successDependency):\n            pytest.skip(f\"Dependency {main.successDependency} not satisfied\")\n    \n        dir_reference = specification.testInfo.referenceDirectory\n        dir_student = specification.testInfo.studentDirectory\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n    \n        #not needed here:\n        #verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n        #competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        \"\"\" Get solutions, measure execution time \"\"\"\n        try:\n            solution_student, exec_time_student = get_solution(monkeymodule, specification, id, main, Solution.student, store_graphics_artefacts)\n            record_property(\"exec_time_student\", exec_time_student)\n            solution_reference, exec_time_reference = get_solution(monkeymodule, specification, id, main, Solution.reference, store_graphics_artefacts)\n            record_property(\"exec_time_reference\", exec_time_reference)\n        except TimeoutError as e:\n            record_property(\"timeout\", True)\n            raise\n    \n        \"\"\" if test is graphics => get saved graphics object as solution \"\"\"\n        if testtype == TypeEnum.graphics:\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\n            TypeEnum.variable,\n            TypeEnum.graphics,\n            TypeEnum.error,\n            TypeEnum.warning,\n            TypeEnum.help,\n        ]:\n            \"\"\" get the student value \"\"\"\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                \"\"\" value not found, try eval \"\"\"\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == QualificationEnum.verifyEqual:\n                \"\"\" get the reference value \"\"\"\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                \"\"\" assert variable-type \"\"\"\n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                \"\"\" assert variable-value \"\"\"\n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n>                   assert val_student == val_reference, failure_msg\nE                   AssertionError: Variable var1 has incorrect value\nE                   assert '123' == '1'\nE                     - 1\nE                     + 123\n\nI:\\PYTHON\\catester\\catester\\tests\\test_class.py:312: AssertionError"
        },
        "user_properties": [
          {
            "exec_time_student": 0.015700101852416992
          },
          {
            "exec_time_reference": 0.007042646408081055
          }
        ],
        "teardown": {
          "duration": 0.0007738000713288784,
          "outcome": "passed"
        }
      },
      {
        "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases2]",
        "lineno": 211,
        "outcome": "passed",
        "keywords": [
          "test_entrypoint[testcases2]",
          "testcases2",
          "CodeabilityPythonTest",
          "test_class.py",
          "tests/__init__.py",
          "catester"
        ],
        "setup": {
          "duration": 0.0005114000523462892,
          "outcome": "passed"
        },
        "call": {
          "duration": 0.000264100031927228,
          "outcome": "passed"
        },
        "user_properties": [
          {
            "exec_time_student": 0
          },
          {
            "exec_time_reference": 0
          }
        ],
        "teardown": {
          "duration": 0.00028469995595514774,
          "outcome": "passed"
        }
      },
      {
        "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases3]",
        "lineno": 211,
        "outcome": "passed",
        "keywords": [
          "test_entrypoint[testcases3]",
          "testcases3",
          "CodeabilityPythonTest",
          "test_class.py",
          "tests/__init__.py",
          "catester"
        ],
        "setup": {
          "duration": 0.0004453000146895647,
          "outcome": "passed"
        },
        "call": {
          "duration": 0.00024800002574920654,
          "outcome": "passed"
        },
        "user_properties": [
          {
            "exec_time_student": 0
          },
          {
            "exec_time_reference": 0
          }
        ],
        "teardown": {
          "duration": 0.0002874999772757292,
          "outcome": "passed"
        }
      },
      {
        "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases4]",
        "lineno": 211,
        "outcome": "skipped",
        "keywords": [
          "test_entrypoint[testcases4]",
          "testcases4",
          "CodeabilityPythonTest",
          "test_class.py",
          "tests/__init__.py",
          "catester"
        ],
        "setup": {
          "duration": 0.0004458000184968114,
          "outcome": "passed"
        },
        "call": {
          "duration": 0.0002589999930933118,
          "outcome": "skipped",
          "longrepr": "('I:\\\\PYTHON\\\\catester\\\\catester\\\\tests\\\\test_class.py', 224, \"Skipped: Dependency ['0', '1'] not satisfied\")"
        },
        "teardown": {
          "duration": 0.00034010002855211496,
          "outcome": "passed"
        }
      }
    ]
  }
}