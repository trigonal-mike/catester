{
  "created": 1704968266.002166,
  "duration": 1.9604222774505615,
  "exitcode": 1,
  "root": "i:\\PYTHON\\catester\\catester",
  "environment": {},
  "summary": {
    "passed": 29,
    "failed": 14,
    "total": 43,
    "collected": 43
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "model/__init__.py",
          "type": "Package"
        },
        {
          "nodeid": "tests/__init__.py",
          "type": "Package"
        }
      ]
    },
    {
      "nodeid": "model/__init__.py",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases0]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases1]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases2]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases3]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases4]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases5]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases6]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases7]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases8]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases9]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases10]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases11]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases12]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases13]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases14]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases15]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases16]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases17]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases18]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases19]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases20]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases21]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases22]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases23]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases24]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases25]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases26]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases27]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases28]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases29]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases30]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases31]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases32]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases33]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases34]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases35]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases36]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases37]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases38]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases39]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases40]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases41]",
          "type": "Function",
          "lineno": 154
        },
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases42]",
          "type": "Function",
          "lineno": 154
        }
      ]
    },
    {
      "nodeid": "tests/test_class.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_class.py::CodeabilityPythonTest",
          "type": "Class"
        }
      ]
    },
    {
      "nodeid": "tests/__init__.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_class.py",
          "type": "Module"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases0]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases0]",
        "testcases0",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-05,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.015191300000878982,
        "outcome": "passed",
        "stdout": "setup_class\nsetup_method\n"
      },
      "call": {
        "duration": 0.0018710000003920868,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            0
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 0
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0005776000034529716,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases1]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases1]",
        "testcases1",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.000767299992730841,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00029329999233596027,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            1
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 1
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0004670000053010881,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases2]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases2]",
        "testcases2",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var3",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007359000010183081,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0003569999971659854,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            2
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 2
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0004434000002220273,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases3]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases3]",
        "testcases3",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var4",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0008003999973880127,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0007523999956902117,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 232,
          "message": "AssertionError: Variable var4 not found in student namespace"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 232,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A8C10>, request = <FixtureRequest for <Function test_entrypoint[testcases3]>>\nrecord_property = <function record_property.<locals>.append_property at 0x000002719258D120>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192595B70>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (0, 3), json_metadata = {'absolute_tolerance': 0.001, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases3]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n>                   val_student = eval(name, solution_student)\n\ntests\\test_class.py:230: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n>   ???\nE   NameError: name 'var4' is not defined\n\n<string>:1: NameError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A8C10>, request = <FixtureRequest for <Function test_entrypoint[testcases3]>>\nrecord_property = <function record_property.<locals>.append_property at 0x000002719258D120>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192595B70>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (0, 3), json_metadata = {'absolute_tolerance': 0.001, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases3]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n>                   raise AssertionError(f\"Variable {name} not found in student namespace\")\nE                   AssertionError: Variable var4 not found in student namespace\n\ntests\\test_class.py:232: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            3
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 3
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0005412999889813364,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases4]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases4]",
        "testcases4",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var5",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007009000109974295,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.000527299998793751,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 254,
          "message": "AssertionError: Variable var5 has incorrect type, expected: <class 'tuple'>, obtained <class 'list'>\nassert <class 'list'> == <class 'tuple'>"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 254,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A88B0>, request = <FixtureRequest for <Function test_entrypoint[testcases4]>>\nrecord_property = <function record_property.<locals>.append_property at 0x000002719258D750>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192693190>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (0, 4), json_metadata = {'absolute_tolerance': 0.001, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases4]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n>               assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\nE               AssertionError: Variable var5 has incorrect type, expected: <class 'tuple'>, obtained <class 'list'>\nE               assert <class 'list'> == <class 'tuple'>\n\ntests\\test_class.py:254: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            4
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 4
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0005560000136028975,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases5]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases5]",
        "testcases5",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var6",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007000999903539196,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0004226000019116327,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 258,
          "message": "AssertionError: Variable var6 has incorrect value\nassert {1, 2, 3} == {1, 2}\n  Extra items in the left set:\n  3\n  Use -v to get more diff"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 258,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A8A30>, request = <FixtureRequest for <Function test_entrypoint[testcases5]>>\nrecord_property = <function record_property.<locals>.append_property at 0x0000027192657250>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x00000271925ABFA0>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (0, 5), json_metadata = {'absolute_tolerance': 0.001, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases5]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n>                   assert val_student == val_reference, failure_msg\nE                   AssertionError: Variable var6 has incorrect value\nE                   assert {1, 2, 3} == {1, 2}\nE                     Extra items in the left set:\nE                     3\nE                     Use -v to get more diff\n\ntests\\test_class.py:258: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            5
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 5
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0006537000008393079,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases6]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases6]",
        "testcases6",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var7",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007870000117691234,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00026529999740887433,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            6
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 6
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.0004025999951409176,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases7]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases7]",
        "testcases7",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Basic",
        "main_description": null,
        "sub_name": "var8",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 0.1,
        "absolute_tolerance": 0.001,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006487999926321208,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00040389999048784375,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            0,
            7
          ]
        },
        {
          "idx_main": 0
        },
        {
          "idx_sub": 7
        },
        {
          "main": "Basic.py"
        }
      ],
      "teardown": {
        "duration": 0.00040439999429509044,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases8]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases8]",
        "testcases8",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x1",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006505000055767596,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0012147999950684607,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            0
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 0
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0004062999942107126,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases9]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases9]",
        "testcases9",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x2",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.000647599998046644,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0002628999936860055,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            1
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 1
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.00045399999362416565,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases10]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases10]",
        "testcases10",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x3",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007183999987319112,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00026889999571722,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            2
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 2
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0004008999967481941,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases11]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases11]",
        "testcases11",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x4",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007523999956902117,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00032469999860040843,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            3
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 3
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0006458999996539205,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases12]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases12]",
        "testcases12",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x5",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0011224000045331195,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0005796000041300431,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            4
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 4
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.000568500006920658,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases13]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases13]",
        "testcases13",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x6",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007442999922204763,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0002992000081576407,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            5
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 5
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0004743999888887629,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases14]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases14]",
        "testcases14",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x7",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006493999972008169,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0004330000083427876,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            6
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 6
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0004572000034386292,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases15]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases15]",
        "testcases15",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x8",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006493000109912828,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0002747999969869852,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            7
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 7
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.00046949999523349106,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases16]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases16]",
        "testcases16",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x9",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006447000050684437,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00024500000290572643,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            8
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 8
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.00046579999616369605,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases17]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases17]",
        "testcases17",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x10",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006430000066757202,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0002451999898767099,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            9
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 9
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.00045780000800732523,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases18]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases18]",
        "testcases18",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x11",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006348000024445355,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00025639998784754425,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            10
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 10
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.000454099994385615,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases19]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases19]",
        "testcases19",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x12",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0010414999997010455,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0008402999956160784,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            11
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 11
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.001132700010202825,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases20]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases20]",
        "testcases20",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x13",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0015951999957906082,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0007925999962026253,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            12
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 12
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0010285000025760382,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases21]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases21]",
        "testcases21",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x14",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.002340799997909926,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0009525999921606854,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            13
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 13
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0010772999958135188,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases22]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases22]",
        "testcases22",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test PythonTypes",
        "main_description": null,
        "sub_name": "x15",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0015755000058561563,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0008311999990837649,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            1,
            14
          ]
        },
        {
          "idx_main": 1
        },
        {
          "idx_sub": 14
        },
        {
          "main": "PythonTypes.py"
        }
      ],
      "teardown": {
        "duration": 0.0011185000039404258,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases23]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases23]",
        "testcases23",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test DateTime",
        "main_description": null,
        "sub_name": "var_date",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.002135099988663569,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.002450100000714883,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            2,
            0
          ]
        },
        {
          "idx_main": 2
        },
        {
          "idx_sub": 0
        },
        {
          "main": "DateTime.py"
        }
      ],
      "teardown": {
        "duration": 0.001320399998803623,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases24]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases24]",
        "testcases24",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test DateTime",
        "main_description": null,
        "sub_name": "var_time",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0010701999999582767,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0006638999911956489,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 268,
          "message": "AssertionError: Variable var_time has incorrect value\nassert datetime.time...7, 45, 184945) == 11:17:45.183948\n  comparison failed\n  Obtained: 11:17:45.184945\n  Expected: 11:17:45.183948"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 268,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A93F0>, request = <FixtureRequest for <Function test_entrypoint[testcases24]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271926F43A0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x00000271925AA800>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (2, 1), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases24]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n>                   assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\nE                   AssertionError: Variable var_time has incorrect value\nE                   assert datetime.time...7, 45, 184945) == 11:17:45.183948\nE                     comparison failed\nE                     Obtained: 11:17:45.184945\nE                     Expected: 11:17:45.183948\n\ntests\\test_class.py:268: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            2,
            1
          ]
        },
        {
          "idx_main": 2
        },
        {
          "idx_sub": 1
        },
        {
          "main": "DateTime.py"
        }
      ],
      "teardown": {
        "duration": 0.0007723000016994774,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases25]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases25]",
        "testcases25",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test DateTime",
        "main_description": null,
        "sub_name": "var_datetime",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.000930900001549162,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0006505999917862937,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 268,
          "message": "AssertionError: Variable var_datetime has incorrect value\nassert datetime.date...7, 45, 184945) == 2024-01-11 11:17:45.183948\n  comparison failed\n  Obtained: 2024-01-11 11:17:45.184945\n  Expected: 2024-01-11 11:17:45.183948"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 268,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A9570>, request = <FixtureRequest for <Function test_entrypoint[testcases25]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271926F49D0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192691AB0>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (2, 2), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases25]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n>                   assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\nE                   AssertionError: Variable var_datetime has incorrect value\nE                   assert datetime.date...7, 45, 184945) == 2024-01-11 11:17:45.183948\nE                     comparison failed\nE                     Obtained: 2024-01-11 11:17:45.184945\nE                     Expected: 2024-01-11 11:17:45.183948\n\ntests\\test_class.py:268: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            2,
            2
          ]
        },
        {
          "idx_main": 2
        },
        {
          "idx_sub": 2
        },
        {
          "main": "DateTime.py"
        }
      ],
      "teardown": {
        "duration": 0.001203799998620525,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases26]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases26]",
        "testcases26",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test DateTime",
        "main_description": null,
        "sub_name": "var_duration",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0011383999953977764,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0006603000074392185,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 268,
          "message": "AssertionError: Variable var_duration has incorrect value\nassert datetime.time...croseconds=10) == 5 days, 3:00:00\n  comparison failed\n  Obtained: 5 days, 3:00:00.000010\n  Expected: 5 days, 3:00:00"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 268,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A96F0>, request = <FixtureRequest for <Function test_entrypoint[testcases26]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271926F5CF0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192596680>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (2, 3), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases26]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n>                   assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\nE                   AssertionError: Variable var_duration has incorrect value\nE                   assert datetime.time...croseconds=10) == 5 days, 3:00:00\nE                     comparison failed\nE                     Obtained: 5 days, 3:00:00.000010\nE                     Expected: 5 days, 3:00:00\n\ntests\\test_class.py:268: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            2,
            3
          ]
        },
        {
          "idx_main": 2
        },
        {
          "idx_sub": 3
        },
        {
          "main": "DateTime.py"
        }
      ],
      "teardown": {
        "duration": 0.0007272000075317919,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases27]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases27]",
        "testcases27",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Matplot",
        "main_description": null,
        "sub_name": "x",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-08,
        "absolute_tolerance": 1e-05,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.001066600001649931,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.5590215000120224,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            3,
            0
          ]
        },
        {
          "idx_main": 3
        },
        {
          "idx_sub": 0
        },
        {
          "main": "Matplot.py"
        }
      ],
      "teardown": {
        "duration": 0.0006502000032924116,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases28]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases28]",
        "testcases28",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Matplot",
        "main_description": null,
        "sub_name": "y",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-08,
        "absolute_tolerance": 1e-05,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0009251000010408461,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.001942700007930398,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 265,
          "message": "AssertionError: Variable y has incorrect value"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 265,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A9630>, request = <FixtureRequest for <Function test_entrypoint[testcases28]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271926F6F80>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192679E10>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (3, 1), json_metadata = {'absolute_tolerance': 1e-05, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases28]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n>                       np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n\ntests\\test_class.py:263: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nargs = (<function assert_allclose.<locals>.compare at 0x00000271926F5AB0>, array([ 1.00000000e-04,  1.00938420e-01,  2.007488...76008,  0.13146699,  0.03083368, -0.07011396,\n       -0.17034683, -0.26884313, -0.36459873, -0.45663749, -0.54402111]))\nkwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-08, atol=1e-05', 'verbose': True}\n\n    @wraps(func)\n    def inner(*args, **kwds):\n        with self._recreate_cm():\n>           return func(*args, **kwds)\nE           AssertionError: \nE           Not equal to tolerance rtol=1e-08, atol=1e-05\nE           \nE           Mismatched elements: 100 / 100 (100%)\nE           Max absolute difference: 0.0001\nE           Max relative difference: 0.00972825\nE            x: array([ 1.000000e-04,  1.009384e-01,  2.007489e-01,  2.985138e-01,\nE                   3.932366e-01,  4.839516e-01,  5.697341e-01,  6.497095e-01,\nE                   7.230626e-01,  7.890455e-01,  8.469856e-01,  8.962922e-01,...\nE            y: array([ 0.      ,  0.100838,  0.200649,  0.298414,  0.393137,  0.483852,\nE                   0.569634,  0.64961 ,  0.722963,  0.788945,  0.846886,  0.896192,\nE                   0.936363,  0.966988,  0.987755,  0.998452,  0.998971,  0.989306,...\n\nC:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:79: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A9630>, request = <FixtureRequest for <Function test_entrypoint[testcases28]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271926F6F80>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192679E10>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (3, 1), json_metadata = {'absolute_tolerance': 1e-05, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases28]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n>                       raise AssertionError(failure_msg)\nE                       AssertionError: Variable y has incorrect value\n\ntests\\test_class.py:265: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            3,
            1
          ]
        },
        {
          "idx_main": 3
        },
        {
          "idx_sub": 1
        },
        {
          "main": "Matplot.py"
        }
      ],
      "teardown": {
        "duration": 0.0007583999977214262,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases29]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases29]",
        "testcases29",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Pandas",
        "main_description": null,
        "sub_name": "df",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007621999975526705,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.02732609999657143,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 260,
          "message": "AssertionError: Variable df has incorrect value\nassert False\n +  where False = <bound method NDFrame.equals of    Column1 Column2  Column3\\n0        1       a     True\\n1        2       B    False\\n2        3       C     True>(   Column1 Column2  Column3\\n0        1       A     True\\n1        2       B    False\\n2        3       C     True)\n +    where <bound method NDFrame.equals of    Column1 Column2  Column3\\n0        1       a     True\\n1        2       B    False\\n2        3       C     True> =    Column1 Column2  Column3\\n0        1       a     True\\n1        2       B    False\\n2        3       C     True.equals"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 260,
            "message": "AssertionError"
          }
        ],
        "stdout": "   Column1 Column2  Column3\n0        1       A     True\n1        2       B    False\n2        3       C     True\n<class 'pandas.core.frame.DataFrame'>\na    1\nb    2\nc    3\ndtype: int64\n<class 'pandas.core.series.Series'>\n   Column1 Column2  Column3\n0        1       a     True\n1        2       B    False\n2        3       C     True\n<class 'pandas.core.frame.DataFrame'>\na    1\nb    2\nc    3\ndtype: int64\n<class 'pandas.core.series.Series'>\n",
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A97B0>, request = <FixtureRequest for <Function test_entrypoint[testcases29]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271965409D0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x00000271964ECD30>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (4, 0), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases29]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n>                   assert val_student.equals(val_reference), failure_msg\nE                   AssertionError: Variable df has incorrect value\nE                   assert False\nE                    +  where False = <bound method NDFrame.equals of    Column1 Column2  Column3\\n0        1       a     True\\n1        2       B    False\\n2        3       C     True>(   Column1 Column2  Column3\\n0        1       A     True\\n1        2       B    False\\n2        3       C     True)\nE                    +    where <bound method NDFrame.equals of    Column1 Column2  Column3\\n0        1       a     True\\n1        2       B    False\\n2        3       C     True> =    Column1 Column2  Column3\\n0        1       a     True\\n1        2       B    False\\n2        3       C     True.equals\n\ntests\\test_class.py:260: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            4,
            0
          ]
        },
        {
          "idx_main": 4
        },
        {
          "idx_sub": 0
        },
        {
          "main": "Pandas.py"
        }
      ],
      "teardown": {
        "duration": 0.0007220999978017062,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases30]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases30]",
        "testcases30",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Pandas",
        "main_description": null,
        "sub_name": "ser",
        "testtype": "variable",
        "qualification": "verifyEqual",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.000811800011433661,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00037460000021383166,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            4,
            1
          ]
        },
        {
          "idx_main": 4
        },
        {
          "idx_sub": 1
        },
        {
          "main": "Pandas.py"
        }
      ],
      "teardown": {
        "duration": 0.0005488000024342909,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases31]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases31]",
        "testcases31",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "matches",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006940000021131709,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0007533000025432557,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            0
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 0
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.00040889999945648015,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases32]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases32]",
        "testcases32",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "contains",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006496999994851649,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0002505000011296943,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            1
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 1
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.0005738000036217272,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases33]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases33]",
        "testcases33",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "startsWith",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.001373299994156696,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0005785999965155497,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            2
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 2
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.000644500003545545,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases34]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases34]",
        "testcases34",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "endsWith",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007042999932309613,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00025779999850783497,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            3
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 3
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.00040129999979399145,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases35]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases35]",
        "testcases35",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "count",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006400999991456047,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0002525000018067658,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            4
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 4
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.00039739999920129776,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases36]",
      "lineno": 154,
      "outcome": "passed",
      "keywords": [
        "test_entrypoint[testcases36]",
        "testcases36",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var1",
        "testtype": "variable",
        "qualification": "regexp",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006372000061674044,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00033819999953266233,
        "outcome": "passed"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            5
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 5
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.00039280000783037394,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases37]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases37]",
        "testcases37",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "matches",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.000641100006760098,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00042219999886583537,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 270,
          "message": "AssertionError: Variable var2 does not match the specified pattern -\nassert '_x1234567890' == '-'\n  - -\n  + _x1234567890"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 270,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A9C30>, request = <FixtureRequest for <Function test_entrypoint[testcases37]>>\nrecord_property = <function record_property.<locals>.append_property at 0x000002719656A680>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x000002719658DDE0>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (5, 6), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases37]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n                    assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\n            elif qualification == \"matches\":\n>               assert str(val_student) == pattern, f\"Variable {name} does not match the specified pattern {pattern}\"\nE               AssertionError: Variable var2 does not match the specified pattern -\nE               assert '_x1234567890' == '-'\nE                 - -\nE                 + _x1234567890\n\ntests\\test_class.py:270: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            6
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 6
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.0005518999969353899,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases38]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases38]",
        "testcases38",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "contains",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007021999917924404,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.00037029999657534063,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 272,
          "message": "AssertionError: Variable var2 does not contain the specified pattern -\nassert -1 > -1\n +  where -1 = <built-in method find of str object at 0x000002719636BC30>('-')\n +    where <built-in method find of str object at 0x000002719636BC30> = '_x1234567890'.find\n +      where '_x1234567890' = str('_x1234567890')"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 272,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A9DB0>, request = <FixtureRequest for <Function test_entrypoint[testcases38]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271965296C0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x00000271963ED9F0>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (5, 7), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases38]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n                    assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\n            elif qualification == \"matches\":\n                assert str(val_student) == pattern, f\"Variable {name} does not match the specified pattern {pattern}\"\n            elif qualification == \"contains\":\n>               assert str(val_student).find(pattern) > -1, f\"Variable {name} does not contain the specified pattern {pattern}\"\nE               AssertionError: Variable var2 does not contain the specified pattern -\nE               assert -1 > -1\nE                +  where -1 = <built-in method find of str object at 0x000002719636BC30>('-')\nE                +    where <built-in method find of str object at 0x000002719636BC30> = '_x1234567890'.find\nE                +      where '_x1234567890' = str('_x1234567890')\n\ntests\\test_class.py:272: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            7
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 7
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.0005865999992238358,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases39]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases39]",
        "testcases39",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "startsWith",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0010221999982604757,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0008092999923974276,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 274,
          "message": "AssertionError: Variable var2 does not start with the specified pattern -\nassert False\n +  where False = <built-in method startswith of str object at 0x000002719636BC30>('-')\n +    where <built-in method startswith of str object at 0x000002719636BC30> = '_x1234567890'.startswith\n +      where '_x1234567890' = str('_x1234567890')"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 274,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925AA050>, request = <FixtureRequest for <Function test_entrypoint[testcases39]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271965292D0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192595000>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (5, 8), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases39]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n                    assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\n            elif qualification == \"matches\":\n                assert str(val_student) == pattern, f\"Variable {name} does not match the specified pattern {pattern}\"\n            elif qualification == \"contains\":\n                assert str(val_student).find(pattern) > -1, f\"Variable {name} does not contain the specified pattern {pattern}\"\n            elif qualification == \"startsWith\":\n>               assert str(val_student).startswith(pattern), f\"Variable {name} does not start with the specified pattern {pattern}\"\nE               AssertionError: Variable var2 does not start with the specified pattern -\nE               assert False\nE                +  where False = <built-in method startswith of str object at 0x000002719636BC30>('-')\nE                +    where <built-in method startswith of str object at 0x000002719636BC30> = '_x1234567890'.startswith\nE                +      where '_x1234567890' = str('_x1234567890')\n\ntests\\test_class.py:274: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            8
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 8
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.0007698999979766086,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases40]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases40]",
        "testcases40",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "endsWith",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.001352099992800504,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0003865999897243455,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 276,
          "message": "AssertionError: Variable var2 does not end with the specified pattern -\nassert False\n +  where False = <built-in method endswith of str object at 0x000002719636BC30>('-')\n +    where <built-in method endswith of str object at 0x000002719636BC30> = '_x1234567890'.endswith\n +      where '_x1234567890' = str('_x1234567890')"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 276,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925AA170>, request = <FixtureRequest for <Function test_entrypoint[testcases40]>>\nrecord_property = <function record_property.<locals>.append_property at 0x000002719652A5F0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192683910>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (5, 9), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases40]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n                    assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\n            elif qualification == \"matches\":\n                assert str(val_student) == pattern, f\"Variable {name} does not match the specified pattern {pattern}\"\n            elif qualification == \"contains\":\n                assert str(val_student).find(pattern) > -1, f\"Variable {name} does not contain the specified pattern {pattern}\"\n            elif qualification == \"startsWith\":\n                assert str(val_student).startswith(pattern), f\"Variable {name} does not start with the specified pattern {pattern}\"\n            elif qualification == \"endsWith\":\n>               assert str(val_student).endswith(pattern), f\"Variable {name} does not end with the specified pattern {pattern}\"\nE               AssertionError: Variable var2 does not end with the specified pattern -\nE               assert False\nE                +  where False = <built-in method endswith of str object at 0x000002719636BC30>('-')\nE                +    where <built-in method endswith of str object at 0x000002719636BC30> = '_x1234567890'.endswith\nE                +      where '_x1234567890' = str('_x1234567890')\n\ntests\\test_class.py:276: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            9
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 9
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.0004954999894835055,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases41]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases41]",
        "testcases41",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "count",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0007371999963652343,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.0003934999986086041,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 278,
          "message": "AssertionError: Variable var2 does not contain the specified pattern - 1-times\nassert 0 == 1\n +  where 0 = <built-in method count of str object at 0x000002719636BC30>('-')\n +    where <built-in method count of str object at 0x000002719636BC30> = '_x1234567890'.count\n +      where '_x1234567890' = str('_x1234567890')"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 278,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925AA200>, request = <FixtureRequest for <Function test_entrypoint[testcases41]>>\nrecord_property = <function record_property.<locals>.append_property at 0x00000271965283A0>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x00000271964C4190>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (5, 10), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases41]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n                    assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\n            elif qualification == \"matches\":\n                assert str(val_student) == pattern, f\"Variable {name} does not match the specified pattern {pattern}\"\n            elif qualification == \"contains\":\n                assert str(val_student).find(pattern) > -1, f\"Variable {name} does not contain the specified pattern {pattern}\"\n            elif qualification == \"startsWith\":\n                assert str(val_student).startswith(pattern), f\"Variable {name} does not start with the specified pattern {pattern}\"\n            elif qualification == \"endsWith\":\n                assert str(val_student).endswith(pattern), f\"Variable {name} does not end with the specified pattern {pattern}\"\n            elif qualification == \"count\":\n>               assert str(val_student).count(pattern) == countRequirement, f\"Variable {name} does not contain the specified pattern {pattern} {countRequirement}-times\"\nE               AssertionError: Variable var2 does not contain the specified pattern - 1-times\nE               assert 0 == 1\nE                +  where 0 = <built-in method count of str object at 0x000002719636BC30>('-')\nE                +    where <built-in method count of str object at 0x000002719636BC30> = '_x1234567890'.count\nE                +      where '_x1234567890' = str('_x1234567890')\n\ntests\\test_class.py:278: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            10
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 10
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.0005416999920271337,
        "outcome": "passed",
        "stdout": "teardown_method\n"
      }
    },
    {
      "nodeid": "tests/test_class.py::CodeabilityPythonTest::test_entrypoint[testcases42]",
      "lineno": 154,
      "outcome": "failed",
      "keywords": [
        "test_entrypoint[testcases42]",
        "testcases42",
        "CodeabilityPythonTest",
        "test_class.py",
        "tests/__init__.py",
        "catester"
      ],
      "metadata": {
        "zzz": {
          "ddd": 123
        },
        "main_name": "Test Strings",
        "main_description": null,
        "sub_name": "var2",
        "testtype": "variable",
        "qualification": "regexp",
        "relative_tolerance": 1e-12,
        "absolute_tolerance": 1e-06,
        "allowed_occuranceRange": null,
        "failure_message": "Some or all tests failed",
        "success_message": "Congratulations! All tests passed",
        "verbosity": null,
        "store_graphics_artefacts": false,
        "competency": null
      },
      "setup": {
        "duration": 0.0006870000070193782,
        "outcome": "passed",
        "stdout": "setup_method\n"
      },
      "call": {
        "duration": 0.000411600005463697,
        "outcome": "failed",
        "crash": {
          "path": "I:\\PYTHON\\catester\\catester\\tests\\test_class.py",
          "lineno": 281,
          "message": "AssertionError: Variable var2 does not match the compiled regular expression from the specified pattern ^.*y.*$\nassert None is not None"
        },
        "traceback": [
          {
            "path": "tests\\test_class.py",
            "lineno": 281,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <tests.test_class.CodeabilityPythonTest object at 0x00000271925A9E70>, request = <FixtureRequest for <Function test_entrypoint[testcases42]>>\nrecord_property = <function record_property.<locals>.append_property at 0x000002719652A320>, monkeymodule = <_pytest.monkeypatch.MonkeyPatch object at 0x0000027192847430>\nconfig = {'abs_path_to_yaml': 'i:\\\\PYTHON\\\\catester\\\\examples\\\\ex1', 'specification': CodeAbilitySpecification(testInfo=CodeAbi...Message=None, verbosity=None, name='var2', value=None, evalString=None, pattern='^.*y.*$', countRequirement=None)])]))}\ntestcases = (5, 11), json_metadata = {'absolute_tolerance': 1e-06, 'allowed_occuranceRange': None, 'competency': None, 'failure_message': 'Some or all tests failed', ...}\nxxxx = <SubRequest 'xxxx' for <Function test_entrypoint[testcases42]>>\n\n    def test_entrypoint(self, request, record_property, monkeymodule, config, testcases, json_metadata, xxxx):\n        idx_main, idx_sub = testcases\n    \n        record_property(\"testcases\", testcases)\n        record_property(\"idx_main\", idx_main)\n        record_property(\"idx_sub\", idx_sub)\n        testsuite: CodeAbilityTestSuite = config[\"testsuite\"]\n        specification: CodeAbilitySpecification = config[\"specification\"]\n        abs_path_to_yaml: str = config[\"abs_path_to_yaml\"]\n    \n        main: CodeAbilityTestCollection = testsuite.properties.tests[idx_main]\n        sub: CodeAbilityTest = main.tests[idx_sub]\n    \n        record_property(\"main\", main.entryPoint)\n        #record_property(\"sub\", sub)\n    \n    \n        ancestors_sub = [sub, main, testsuite.properties]\n        ancestors_main = [main, testsuite.properties]\n    \n        qualification = get_inherited_property(\"qualification\", ancestors_sub, None)\n        relative_tolerance = get_inherited_property(\"relativeTolerance\", ancestors_sub, 0)\n        absolute_tolerance = get_inherited_property(\"absoluteTolerance\", ancestors_sub, 0)\n        allowed_occuranceRange = get_inherited_property(\"allowedOccuranceRange\", ancestors_sub, None)\n        failure_message = get_inherited_property(\"failureMessage\", ancestors_sub, None)\n        success_message = get_inherited_property(\"successMessage\", ancestors_sub, None)\n        verbosity = get_inherited_property(\"verbosity\", ancestors_sub, None)\n    \n        store_graphics_artefacts = get_inherited_property(\"storeGraphicsArtefacts\", ancestors_main, False)\n        competency = get_inherited_property(\"competency\", ancestors_main, None)\n    \n        testtype = main.type\n        file = main.file\n        id = main.id if main.id is not None else str(idx_main + 1)\n    \n        name = sub.name\n        value = sub.value\n        evalString = sub.evalString\n        pattern = sub.pattern\n        countRequirement = sub.countRequirement\n        #options = sub.options\n        #verificationFunction = sub.verificationFunction\n        #json_metadata['sub'] = sub\n        json_metadata['main_name'] = main.name\n        json_metadata['main_description'] = main.description\n        json_metadata['sub_name'] = name\n        json_metadata['testtype'] = testtype\n        json_metadata['qualification'] = qualification\n        json_metadata['relative_tolerance'] = relative_tolerance\n        json_metadata['absolute_tolerance'] = absolute_tolerance\n        json_metadata['allowed_occuranceRange'] = allowed_occuranceRange\n        json_metadata['failure_message'] = failure_message\n        json_metadata['success_message'] = success_message\n        json_metadata['verbosity'] = verbosity\n        json_metadata['store_graphics_artefacts'] = store_graphics_artefacts\n        json_metadata['competency'] = competency\n    \n        _outcomes = xxxx.config.stash[outcomes]\n        #pytest.skip(\"Dependency not satisfied\")\n    \n        solution_reference = get_solution(monkeymodule, config, id, main, Solution.reference, store_graphics_artefacts)\n        solution_student = get_solution(monkeymodule, config, id, main, Solution.student, store_graphics_artefacts)\n    \n        # if test is graphics => get saved graphics object as solution\n        if testtype == \"graphics\":\n            solution_student = solution_student[\"_graphics_object_\"]\n            solution_reference = solution_reference[\"_graphics_object_\"]\n    \n        if testtype in [\"variable\", \"graphics\", \"error\", \"warning\", \"help\"]:\n            # student value\n            if name in solution_student:\n                val_student = solution_student[name]\n            else:\n                # value not found, try eval\n                try:\n                    val_student = eval(name, solution_student)\n                except Exception as e:\n                    raise AssertionError(f\"Variable {name} not found in student namespace\")\n    \n            if qualification == \"verifyEqual\":\n                # reference value\n                if value is not None:\n                    val_reference = value\n                elif evalString is not None:\n                    try:\n                        val_reference = eval(evalString)\n                    except Exception as e:\n                        pytest.skip(reason=\"Evaluation of 'evalString' not possible\")\n                else:\n                    if name in solution_reference:\n                        val_reference = solution_reference[name]\n                    else:\n                        try:\n                            val_reference = eval(name, solution_reference)\n                        except Exception as e:\n                            raise AssertionError(f\"Variable {name} not found in reference namespace\")\n    \n                type_student = type(val_student)\n                type_reference = type(val_reference)\n                assert type_student == type_reference, f\"Variable {name} has incorrect type, expected: {type_reference}, obtained {type_student}\"\n    \n                failure_msg = f\"Variable {name} has incorrect value\"\n                if isinstance(val_student, (str, set, frozenset)):\n                    assert val_student == val_reference, failure_msg\n                elif isinstance(val_student, (DataFrame, Series)):\n                    assert val_student.equals(val_reference), failure_msg\n                elif isinstance(val_student, np.ndarray):\n                    try:\n                        np.testing.assert_allclose(val_student, val_reference, rtol=relative_tolerance, atol=absolute_tolerance)\n                    except AssertionError as e:\n                        raise AssertionError(failure_msg)\n                else:\n                    \"\"\"attention: pytest.approx() does not support nested data structures\"\"\"\n                    assert val_student == pytest.approx(val_reference, rel=relative_tolerance, abs=absolute_tolerance), failure_msg\n            elif qualification == \"matches\":\n                assert str(val_student) == pattern, f\"Variable {name} does not match the specified pattern {pattern}\"\n            elif qualification == \"contains\":\n                assert str(val_student).find(pattern) > -1, f\"Variable {name} does not contain the specified pattern {pattern}\"\n            elif qualification == \"startsWith\":\n                assert str(val_student).startswith(pattern), f\"Variable {name} does not start with the specified pattern {pattern}\"\n            elif qualification == \"endsWith\":\n                assert str(val_student).endswith(pattern), f\"Variable {name} does not end with the specified pattern {pattern}\"\n            elif qualification == \"count\":\n                assert str(val_student).count(pattern) == countRequirement, f\"Variable {name} does not contain the specified pattern {pattern} {countRequirement}-times\"\n            elif qualification == \"regexp\":\n                result = re.match(re.compile(fr\"{pattern}\"), str(val_student))\n>               assert result is not None, f\"Variable {name} does not match the compiled regular expression from the specified pattern {pattern}\"\nE               AssertionError: Variable var2 does not match the compiled regular expression from the specified pattern ^.*y.*$\nE               assert None is not None\n\ntests\\test_class.py:281: AssertionError"
      },
      "user_properties": [
        {
          "testcases": [
            5,
            11
          ]
        },
        {
          "idx_main": 5
        },
        {
          "idx_sub": 11
        },
        {
          "main": "Strings.py"
        }
      ],
      "teardown": {
        "duration": 0.002212499995948747,
        "outcome": "passed",
        "stdout": "teardown_method\nteardown\nteardown_class\n"
      }
    }
  ],
  "_duration": 1.9604222774505615,
  "_metadata": {
    "Python": "3.10.11",
    "Platform": "Windows-10-10.0.19045-SP0",
    "Packages": {
      "pytest": "7.4.3",
      "pluggy": "1.3.0"
    },
    "Plugins": {
      "json-report": "1.5.0",
      "metadata": "3.0.0"
    },
    "specyamlfile": "i:\\PYTHON\\catester\\examples\\ex1\\specification.yaml",
    "testyamlfile": "i:\\PYTHON\\catester\\examples\\ex1\\test1.yaml"
  },
  "_timestamp": "2024-01-11 11:17:46.002166",
  "_type": "python",
  "_version": "1.0",
  "_name": "Python Test suite",
  "_status": "COMPLETED",
  "_result": "ExitCode.TESTS_FAILED",
  "_tests": [
    {
      "name": "Test Basic",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Basic",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Basic",
      "variable": "var3",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Basic",
      "variable": "var4",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Basic",
      "variable": "var5",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Basic",
      "variable": "var6",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Basic",
      "variable": "var7",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Basic",
      "variable": "var8",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x2",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x3",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x4",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x5",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x6",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x7",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x8",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x9",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x10",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x11",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x12",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x13",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x14",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test PythonTypes",
      "variable": "x15",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test DateTime",
      "variable": "var_date",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test DateTime",
      "variable": "var_time",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test DateTime",
      "variable": "var_datetime",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test DateTime",
      "variable": "var_duration",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Matplot",
      "variable": "x",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Matplot",
      "variable": "y",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Pandas",
      "variable": "df",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Pandas",
      "variable": "ser",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var1",
      "status": "COMPLETED",
      "result": "PASSED",
      "details": "Congratulations! All tests passed"
    },
    {
      "name": "Test Strings",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Strings",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Strings",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Strings",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Strings",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    },
    {
      "name": "Test Strings",
      "variable": "var2",
      "status": "COMPLETED",
      "result": "FAILED",
      "details": "Some or all tests failed"
    }
  ]
}